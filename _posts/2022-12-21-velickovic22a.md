---
abstract: 'Neural networks leverage robust internal representations in order to generalise.
  Learning them is difficult, and often requires a large training set that covers
  the data distribution densely. We study a common setting where our task is not purely
  opaque. Indeed, very often we may have access to information about the underlying
  system (e.g. that observations must obey certain laws of physics) that any "tabula
  rasa" neural network would need to re-learn from scratch, penalising performance.
  We incorporate this information into a pre-trained reasoning module, and investigate
  its role in shaping the discovered representations in diverse self-supervised learning
  settings from pixels. Our approach paves the way for a new class of representation
  learning, grounded in algorithmic priors. '
openreview: QBGYYu3l3dG
section: Poster Presentations
title: Reasoning-Modulated Representations
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: velickovic22a
month: 0
tex_title: Reasoning-Modulated Representations
firstpage: '50:1'
lastpage: '50:17'
page: 50:1-50:17
order: 50
cycles: false
bibtex_author: Veli{\v c}kovi{\' c}, Petar and Bo{\v s}njak, Matko and Kipf, Thomas
  and Lerchner, Alexander and Hadsell, Raia and Pascanu, Razvan and Blundell, Charles
author:
- given: Petar
  family: Veličković
- given: Matko
  family: Bošnjak
- given: Thomas
  family: Kipf
- given: Alexander
  family: Lerchner
- given: Raia
  family: Hadsell
- given: Razvan
  family: Pascanu
- given: Charles
  family: Blundell
date: 2022-12-21
address:
container-title: Proceedings of the First Learning on Graphs Conference
volume: '198'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 12
  - 21
pdf: https://proceedings.mlr.press/v198/velickovic22a/velickovic22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
